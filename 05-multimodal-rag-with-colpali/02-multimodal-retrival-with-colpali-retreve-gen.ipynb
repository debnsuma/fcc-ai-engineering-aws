{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.models import ColQwen2, ColQwen2Processor\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import torch\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the model and processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA/MPS is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"{device = }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"vidore/colqwen2-v0.1\"\n",
    "model = ColQwen2.from_pretrained(\n",
    "                pretrained_model_name_or_path=model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=device, \n",
    "                cache_dir=\"./model_cache\"\n",
    "            )\n",
    "\n",
    "processor = ColQwen2Processor.from_pretrained(\n",
    "                pretrained_model_name_or_path=model_name,\n",
    "                cache_dir=\"./model_cache\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting the model to evaluation mode\n",
    "model.eval()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# Downloading the dataset \n",
    "url = \"https://arxiv.org/pdf/1706.03762.pdf\"\n",
    "\n",
    "# Set the filename and filepath\n",
    "filename = \"test.pdf\"\n",
    "filepath = os.path.join(\"data\", filename)\n",
    "\n",
    "# Create the data directory if it doesn't exist\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Download the file\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with open(filepath, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"File downloaded successfully: {filepath}\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Local file path\n",
    "# filepath = \"data/lec_04.pdf\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting PDF to Images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "import pymupdf\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Define the function to process each page of the PDF\n",
    "def process_page_images(page, page_num, base_dir):\n",
    "    # Create a pixmap from the PDF page\n",
    "    pix = page.get_pixmap()\n",
    "\n",
    "    # Define the path where the image will be saved\n",
    "    page_path = os.path.join(base_dir, f\"page_{page_num:03d}.jpeg\")\n",
    "\n",
    "    # Save the pixmap as a JPEG image\n",
    "    pix.save(page_path)\n",
    "\n",
    "    # Open the saved image file and convert it to a base64 string\n",
    "    with open(page_path, 'rb') as file:\n",
    "        encoded_image = base64.b64encode(file.read()).decode('utf8')\n",
    "\n",
    "    # Convert the base64 string back to a bytes object and create a PIL image\n",
    "    image_data = BytesIO(base64.b64decode(encoded_image))\n",
    "    page_image_pil = Image.open(image_data)\n",
    "\n",
    "    # Return the PIL image object\n",
    "    return page_image_pil, page_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = pymupdf.open(filepath)\n",
    "num_pages = len(doc)\n",
    "output_dir = \"data/processed_page_images\"\n",
    "\n",
    "images = []\n",
    "images_paths = []\n",
    "\n",
    "# Make sure the output directory exists\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each page of the PDF\n",
    "for page_num in tqdm(range(num_pages), desc=\"Processing PDF pages\"):\n",
    "    page = doc[page_num]\n",
    "    image, page_path = process_page_images(page, page_num, output_dir)\n",
    "    images.append(image)\n",
    "    images_paths.append(page_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the images into a dataloader\n",
    "dataloader = DataLoader(\n",
    "                            dataset=images,\n",
    "                            batch_size=1,\n",
    "                            shuffle=False,\n",
    "                            collate_fn=lambda x: processor.process_images(x),\n",
    "                        )\n",
    "\n",
    "images_embeddings  = []\n",
    "\n",
    "for batch_doc in tqdm(dataloader):\n",
    "    with torch.no_grad():\n",
    "        batch_doc = {k: v.to(model.device) for k, v in batch_doc.items()}\n",
    "        embeddings_doc = model(**batch_doc)\n",
    "    images_embeddings.extend(list(torch.unbind(embeddings_doc.to(\"cpu\"))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_results(query, topk):\n",
    "    batch_queries = processor.process_queries([query]).to(model.device)\n",
    "\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        query_embeddings = model(**batch_queries)\n",
    "\n",
    "    scores = processor.score_multi_vector(query_embeddings, images_embeddings)\n",
    "    scores = scores.squeeze(0)\n",
    "\n",
    "    # get top-k scores\n",
    "    close_vectors_id = scores.topk(topk).indices.tolist()\n",
    "    \n",
    "    return close_vectors_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is position embedding in transformer models?\"\n",
    "k = 6\n",
    "\n",
    "context_ids = get_results(query=query, topk=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_images_in_grid(image_ids, images):\n",
    "    # Number of images\n",
    "    num_images = len(image_ids)\n",
    "    \n",
    "    # Define the number of columns for the grid\n",
    "    cols = 3\n",
    "    # Calculate the number of rows needed\n",
    "    rows = (num_images + cols - 1) // cols\n",
    "    \n",
    "    # Create a figure with subplots in a grid\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 6))\n",
    "    \n",
    "    # Flatten the axes array for easier iteration\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Loop through the images and their corresponding axes\n",
    "    for ax, image_id in zip(axes, image_ids):\n",
    "        # Resize the image\n",
    "        shrink_factor = (images[image_id].size[0] / 1024)\n",
    "        resized_image = images[image_id].resize((int(images[image_id].size[0] / shrink_factor), \n",
    "                                                 int(images[image_id].size[1] / shrink_factor)))\n",
    "        \n",
    "        # Display the image in the respective subplot\n",
    "        ax.imshow(resized_image)\n",
    "        # Set the title for each subplot\n",
    "        rank = image_ids.index(image_id) + 1\n",
    "        ax.set_title(f'Rank {rank}')\n",
    "        # Hide grid lines\n",
    "        ax.grid(False)\n",
    "        # Hide axes ticks\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    \n",
    "    # Hide unused axes if any\n",
    "    for ax in axes[len(image_ids):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "display_images_in_grid(context_ids, images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Closest top K tokens \n",
    "# context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # All image embeddings \n",
    "# len(images_embeddings), images_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # All images in PIL - JpegImageFile format \n",
    "# len(images), images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from colpali_engine.interpretability import get_similarity_maps_from_embeddings\n",
    "from colpali_engine.interpretability import plot_similarity_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_similarity_map(image, query, model, processor):\n",
    "\n",
    "    # Get the device \n",
    "    device = model.device\n",
    "    \n",
    "    # Prreprocess inputs\n",
    "    batch_images = processor.process_images([image]).to(device)\n",
    "    batch_queries = processor.process_queries([query]).to(device)\n",
    "    \n",
    "    # Forward passes\n",
    "    with torch.no_grad():\n",
    "        image_embeddings = model.forward(**batch_images)\n",
    "        query_embeddings = model.forward(**batch_queries)\n",
    "    \n",
    "    \n",
    "    # Get the number of image patches\n",
    "    n_patches = processor.get_n_patches(image_size=image.size, \n",
    "                                        patch_size=model.patch_size,\n",
    "                                        spatial_merge_size=model.spatial_merge_size)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Get the tensor mask to filter out the embeddings that are not related to the image\n",
    "    image_mask = processor.get_image_mask(batch_images)\n",
    "    \n",
    "    # Generate the similarity maps\n",
    "    batched_similarity_maps = get_similarity_maps_from_embeddings(\n",
    "                                                                    image_embeddings=image_embeddings,\n",
    "                                                                    query_embeddings=query_embeddings,\n",
    "                                                                    n_patches=n_patches,\n",
    "                                                                    image_mask=image_mask,\n",
    "                                                                )\n",
    "    \n",
    "    # Get the similarity map for our (only) input image\n",
    "    similarity_maps = batched_similarity_maps[0]  # (query_length, n_patches_x, n_patches_y)\n",
    "    \n",
    "    # Tokenize the query\n",
    "    query_tokens = processor.tokenizer.tokenize(query)\n",
    "    query_tokens = [item.replace('Ä ', '') for item in query_tokens]\n",
    "    \n",
    "    # Picking a random token \n",
    "    token_idx = np.random.choice(len(query_tokens))\n",
    "    \n",
    "    # Get the similarity map for our (only) input image\n",
    "    fig, ax = plot_similarity_map(image, \n",
    "                                  similarity_maps[token_idx],\n",
    "                                  figsize=(8, 8),\n",
    "                                  show_colorbar=False)\n",
    "    \n",
    "    max_sim_score = similarity_maps[token_idx, :, :].max().item()\n",
    "    ax.set_title(f\"Token #{token_idx}: `{query_tokens[token_idx]}`. MaxSim score: {max_sim_score:.2f}\", fontsize=14)\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figs = []\n",
    "for idx in context_ids:\n",
    "    image = images[idx]\n",
    "    fig = visualize_similarity_map(image, query, model, processor)\n",
    "    figs.append(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_encode_image(image_path: str):\n",
    "\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        \n",
    "    base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    # Determine the image format (supported formats: jpg, jpeg, png, gif, webp)\n",
    "    image_format = Image.open(image_path).format.lower()\n",
    "\n",
    "    message_content = {\n",
    "                    \"image\": {\n",
    "                        \"format\": image_format,\n",
    "                        \"source\": {\"bytes\": image_bytes},\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    return message_content\n",
    "\n",
    "\n",
    "def send_images_to_model_using_converse(matched_items: list, query: str, model_id: str):\n",
    "\n",
    "    system_prompt = 'You are a helpful assistant for question answering. Given the context, answer the question.'\n",
    "\n",
    "    image_list = []\n",
    "    for image_path in matched_items:\n",
    "        image_list.append({\n",
    "            \"image_path\": image_path, \n",
    "        })\n",
    "\n",
    "    content_list = []\n",
    "    for img in image_list:\n",
    "        message_content = read_and_encode_image(img['image_path'])\n",
    "        content_list.append(message_content)\n",
    "    \n",
    "    content_list.append({\"text\": query})\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    # Define a \"user\" message including both the image and a text prompt.\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_list,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Configure the inference parameters.\n",
    "    inf_params = {\"temperature\": .3, \"maxTokens\": 5000}\n",
    "    \n",
    "    # Initialize the Bedrock client\n",
    "    client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=model_id, \n",
    "        messages=messages,\n",
    "        system=system, \n",
    "        inferenceConfig=inf_params\n",
    "    )\n",
    "    \n",
    "    # Print Response\n",
    "    output_message = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    return output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_items = [images_paths[idx] for idx in context_ids]\n",
    "matched_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRO_MODEL_ID = \"amazon.nova-pro-v1:0\"\n",
    "LITE_MODEL_ID = \"amazon.nova-lite-v1:0\"\n",
    "MICRO_MODEL_ID = \"amazon.nova-micro-v1:0\"\n",
    "\n",
    "response = send_images_to_model_using_converse(matched_items=matched_items, query=query, model_id=PRO_MODEL_ID)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def cleanup_memory(device = device):\n",
    "    \"\"\"Clean up memory by deleting variables and running garbage collection for CPU, CUDA, or MPS\"\"\"\n",
    "    import gc\n",
    "    \n",
    "    variables_to_clean = [\n",
    "        'query_content',\n",
    "        'query_tokens',\n",
    "        'batch_queries',\n",
    "        'batched_similarity_maps',\n",
    "        'similarity_maps',\n",
    "        'image_mask',\n",
    "        'n_patches',\n",
    "        'im'\n",
    "    ]\n",
    "    \n",
    "    # Delete variables if they exist in global scope\n",
    "    for var in variables_to_clean:\n",
    "        if var in globals():\n",
    "            del globals()[var]\n",
    "    \n",
    "    # Force garbage collection\n",
    "    gc.collect()\n",
    "    \n",
    "    # Clear CUDA cache if using CUDA\n",
    "    if device == 'cuda':\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    else:\n",
    "        torch.mps.empty_cache()\n",
    "\n",
    "    # Delete the processed folder\n",
    "    if os.path.exists(\"data/processed_page_images\"):\n",
    "        shutil.rmtree(\"data/processed_page_images\")\n",
    "\n",
    "    # Delete the model cache\n",
    "    if os.path.exists(\"model_cache\"):\n",
    "        shutil.rmtree(\"model_cache\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run cleanup\n",
    "# cleanup_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
