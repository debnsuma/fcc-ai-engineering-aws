{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Multimodal Agentic RAG with Document Retrieval (ColPali), Vision Language Model (ColQwen2), Amazon Nova and CrewAI\n",
    "</h1>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    Loading the PDF files (Dataset)\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Download pdfs from different sources \n",
    "def download_pdf(pdfs, output_dir):\n",
    "    for name, url in pdfs.items():\n",
    "        response = requests.get(url)\n",
    "        pdf_path = os.path.join(output_dir, f\"{name}.pdf\")\n",
    "\n",
    "        with open(pdf_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "\n",
    "        print(f\"Downloaded {name} to {pdf_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded Transformers to ./data/Transformers.pdf\n",
      "Downloaded DSPy to ./data/DSPy.pdf\n",
      "Downloaded ColPali to ./data/ColPali.pdf\n"
     ]
    }
   ],
   "source": [
    "pdfs = {\n",
    "    \"Transformers\": \"https://arxiv.org/pdf/1706.03762.pdf\",  \n",
    "    \"DSPy\": \"https://arxiv.org/pdf/2310.03714.pdf\", \n",
    "    \"ColPali\": \"https://arxiv.org/pdf/2407.01449.pdf\",\n",
    "}\n",
    "\n",
    "# Downloading the PDF files \n",
    "input_path = \"./data\"\n",
    "os.makedirs(input_path, exist_ok=True)\n",
    "\n",
    "download_pdf(pdfs, input_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Build the Retrieval Model \n",
    "</h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from byaldi import RAGMultiModalModel\n",
    "from PIL import Image\n",
    "from pdf2image import convert_from_path\n",
    "import torch\n",
    "\n",
    "class ImageRetriever:\n",
    "    def __init__(self, model_name, device, output_dir=\"matched_images\"):\n",
    "        \"\"\"\n",
    "        Initialize the RAG model for multimodal retrieval.\n",
    "        \n",
    "        :param model_name: Name of the pretrained model.\n",
    "        :param device: Device to run the model on ('cpu' or 'cuda').\n",
    "        :param output_dir: Directory where matched images will be saved.\n",
    "        \"\"\"\n",
    "        self.myRAG = RAGMultiModalModel.from_pretrained(model_name, device=device)\n",
    "        self.output_dir = output_dir\n",
    "        self.indexed = False  # Flag to check if indexing is already done\n",
    "        self.all_images = {}  # Dictionary to store images from PDFs\n",
    "\n",
    "    def convert_pdfs_to_images(self, pdf_folder):\n",
    "        \"\"\"\n",
    "        Convert all PDFs in the given folder into images and store them.\n",
    "\n",
    "        :param pdf_folder: Path to the folder containing PDFs.\n",
    "        \"\"\"\n",
    "        pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "        all_images = {}\n",
    "\n",
    "        if not pdf_files:\n",
    "            print(\"No PDF files found in the given folder.\")\n",
    "\n",
    "        for doc_id, pdf_file in enumerate(pdf_files):\n",
    "            pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "            images = convert_from_path(pdf_path)\n",
    "            all_images[doc_id] = images  # Map doc_id to the images of this PDF\n",
    "\n",
    "        self.all_images = all_images  # Store in class for retrieval\n",
    "        print(f\"Converted {len(pdf_files)} PDFs to images.\")\n",
    "\n",
    "    def index_documents(self, input_path, index_name=\"research_papers\"):\n",
    "        \"\"\"\n",
    "        Convert PDFs to images (if not already converted) and index the documents.\n",
    "\n",
    "        :param input_path: Directory where the documents are stored.\n",
    "        :param index_name: Name of the index.\n",
    "        \"\"\"\n",
    "        if self.indexed:\n",
    "            print(\"Documents are already indexed. Skipping indexing...\")\n",
    "            return\n",
    "\n",
    "        # Convert PDFs to images before indexing\n",
    "        self.convert_pdfs_to_images(input_path)\n",
    "\n",
    "        # Index the documents\n",
    "        self.myRAG.index(input_path=input_path, \n",
    "                         index_name=index_name, \n",
    "                         store_collection_with_index=False, \n",
    "                         overwrite=True)  # Avoid overwriting existing index\n",
    "\n",
    "        self.indexed = True  # Set flag to True to prevent re-indexing\n",
    "        print(\"Indexing completed successfully.\")\n",
    "\n",
    "    def get_matched_images(self, results):\n",
    "        \"\"\"\n",
    "        Retrieve images that match the search results.\n",
    "\n",
    "        :param results: List of search results containing doc_id and page_num.\n",
    "        :return: List of PIL images.\n",
    "        \"\"\"\n",
    "        matched_images = []\n",
    "        for result in results:\n",
    "            doc_id = result[\"doc_id\"]\n",
    "            page_num = result[\"page_num\"]\n",
    "\n",
    "            if doc_id in self.all_images and len(self.all_images[doc_id]) >= page_num:\n",
    "                matched_images.append(self.all_images[doc_id][page_num - 1])\n",
    "            else:\n",
    "                print(f\"Warning: Image for doc_id {doc_id} and page_num {page_num} not found.\")\n",
    "\n",
    "        return matched_images\n",
    "\n",
    "    def save_images_as_png(self, image_list):\n",
    "        \"\"\"\n",
    "        Convert and save PIL images as PNG format.\n",
    "\n",
    "        :param image_list: List of PIL image objects.\n",
    "        :return: List of file paths where images are saved.\n",
    "        \"\"\"\n",
    "        os.makedirs(self.output_dir, exist_ok=True)  # Ensure output directory exists\n",
    "        file_paths = []  # List to store saved file paths\n",
    "\n",
    "        for idx, img in enumerate(image_list):\n",
    "            file_path = os.path.join(self.output_dir, f\"image_{idx + 1}.png\")\n",
    "            img.save(file_path, format=\"PNG\")\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "        return file_paths\n",
    "\n",
    "    def retrieve_images(self, text_query, k=5):\n",
    "        \"\"\"\n",
    "        Perform a search and retrieve matched image paths.\n",
    "\n",
    "        :param text_query: Query string to search for.\n",
    "        :param k: Number of results to return.\n",
    "        :return: List of file paths to matched images.\n",
    "        \"\"\"\n",
    "        if not self.indexed:\n",
    "            raise ValueError(\"Documents are not indexed. Please call `index_documents()` first.\")\n",
    "\n",
    "        # Perform search using the query\n",
    "        results = self.myRAG.search(text_query, k=k)\n",
    "\n",
    "        # Retrieve matched images from the results\n",
    "        matched_images = self.get_matched_images(results)\n",
    "\n",
    "        if not matched_images:\n",
    "            print(\"No matching images found.\")\n",
    "            return []\n",
    "\n",
    "        # Save and return paths to the images\n",
    "        return self.save_images_as_png(matched_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verbosity is set to 1 (active). Pass verbose=0 to make quieter.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51c6f4e7f022441fa17b843cff3c3dee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Usage Example\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "model_name = \"vidore/colpali-v1.2\"\n",
    "input_path = \"./data\"  # Folder containing the PDFs\n",
    "\n",
    "# Initialize the ImageRetriever class\n",
    "image_retriever = ImageRetriever(model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 3 PDFs to images.\n",
      "overwrite is on. Deleting existing index research_papers to build a new one.\n",
      "Indexing file: data/DSPy.pdf\n",
      "Added page 1 of document 0 to index.\n",
      "Added page 2 of document 0 to index.\n",
      "Added page 3 of document 0 to index.\n",
      "Added page 4 of document 0 to index.\n",
      "Added page 5 of document 0 to index.\n",
      "Added page 6 of document 0 to index.\n",
      "Added page 7 of document 0 to index.\n",
      "Added page 8 of document 0 to index.\n",
      "Added page 9 of document 0 to index.\n",
      "Added page 10 of document 0 to index.\n",
      "Added page 11 of document 0 to index.\n",
      "Added page 12 of document 0 to index.\n",
      "Added page 13 of document 0 to index.\n",
      "Added page 14 of document 0 to index.\n",
      "Added page 15 of document 0 to index.\n",
      "Added page 16 of document 0 to index.\n",
      "Added page 17 of document 0 to index.\n",
      "Added page 18 of document 0 to index.\n",
      "Added page 19 of document 0 to index.\n",
      "Added page 20 of document 0 to index.\n",
      "Added page 21 of document 0 to index.\n",
      "Added page 22 of document 0 to index.\n",
      "Added page 23 of document 0 to index.\n",
      "Added page 24 of document 0 to index.\n",
      "Added page 25 of document 0 to index.\n",
      "Added page 26 of document 0 to index.\n",
      "Added page 27 of document 0 to index.\n",
      "Added page 28 of document 0 to index.\n",
      "Added page 29 of document 0 to index.\n",
      "Added page 30 of document 0 to index.\n",
      "Added page 31 of document 0 to index.\n",
      "Added page 32 of document 0 to index.\n",
      "Index exported to .byaldi/research_papers\n",
      "Indexing file: data/ColPali.pdf\n",
      "Added page 1 of document 1 to index.\n",
      "Added page 2 of document 1 to index.\n",
      "Added page 3 of document 1 to index.\n",
      "Added page 4 of document 1 to index.\n",
      "Added page 5 of document 1 to index.\n",
      "Added page 6 of document 1 to index.\n",
      "Added page 7 of document 1 to index.\n",
      "Added page 8 of document 1 to index.\n",
      "Added page 9 of document 1 to index.\n",
      "Added page 10 of document 1 to index.\n",
      "Added page 11 of document 1 to index.\n",
      "Added page 12 of document 1 to index.\n",
      "Added page 13 of document 1 to index.\n",
      "Added page 14 of document 1 to index.\n",
      "Added page 15 of document 1 to index.\n",
      "Added page 16 of document 1 to index.\n",
      "Added page 17 of document 1 to index.\n",
      "Added page 18 of document 1 to index.\n",
      "Added page 19 of document 1 to index.\n",
      "Added page 20 of document 1 to index.\n",
      "Added page 21 of document 1 to index.\n",
      "Added page 22 of document 1 to index.\n",
      "Added page 23 of document 1 to index.\n",
      "Added page 24 of document 1 to index.\n",
      "Added page 25 of document 1 to index.\n",
      "Added page 26 of document 1 to index.\n",
      "Index exported to .byaldi/research_papers\n",
      "Indexing file: data/Transformers.pdf\n",
      "Added page 1 of document 2 to index.\n",
      "Added page 2 of document 2 to index.\n",
      "Added page 3 of document 2 to index.\n",
      "Added page 4 of document 2 to index.\n",
      "Added page 5 of document 2 to index.\n",
      "Added page 6 of document 2 to index.\n",
      "Added page 7 of document 2 to index.\n",
      "Added page 8 of document 2 to index.\n",
      "Added page 9 of document 2 to index.\n",
      "Added page 10 of document 2 to index.\n",
      "Added page 11 of document 2 to index.\n",
      "Added page 12 of document 2 to index.\n",
      "Added page 13 of document 2 to index.\n",
      "Added page 14 of document 2 to index.\n",
      "Added page 15 of document 2 to index.\n",
      "Index exported to .byaldi/research_papers\n",
      "Index exported to .byaldi/research_papers\n",
      "Indexing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "# Index documents (automatically converts PDFs to images before indexing)\n",
    "image_retriever.index_documents(input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['matched_images/image_1.png', 'matched_images/image_2.png', 'matched_images/image_3.png', 'matched_images/image_4.png', 'matched_images/image_5.png']\n"
     ]
    }
   ],
   "source": [
    "text_query = \"What is proposed in the DSPy programming model?\"\n",
    "\n",
    "# Now, perform searches multiple times without re-indexing\n",
    "matched_image_paths = image_retriever.retrieve_images(text_query)\n",
    "\n",
    "# Print the matched image paths\n",
    "print(matched_image_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Generation with Bedrock Nova\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import base64\n",
    "\n",
    "def read_and_encode_image(image_path: str):\n",
    "\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        \n",
    "    base64_encoded = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    # Determine the image format (supported formats: jpg, jpeg, png, gif, webp)\n",
    "    image_format = Image.open(image_path).format.lower()\n",
    "\n",
    "    message_content = {\n",
    "                    \"image\": {\n",
    "                        \"format\": image_format,\n",
    "                        \"source\": {\"bytes\": image_bytes},\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    return message_content\n",
    "\n",
    "\n",
    "def send_images_to_model_using_converse(matched_items: list, query: str, model_id: str):\n",
    "\n",
    "    system_prompt = 'You are a helpful assistant for question answering. Given the context, answer the question in details, and if needed format the code in markdown.'\n",
    "\n",
    "    image_list = []\n",
    "    for image_path in matched_items:\n",
    "        image_list.append({\n",
    "            \"image_path\": image_path, \n",
    "        })\n",
    "\n",
    "    content_list = []\n",
    "    for img in image_list:\n",
    "        message_content = read_and_encode_image(img['image_path'])\n",
    "        content_list.append(message_content)\n",
    "    \n",
    "    content_list.append({\"text\": query})\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    \n",
    "    # Define a \"user\" message including both the image and a text prompt.\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_list,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Configure the inference parameters.\n",
    "    inf_params = {\"temperature\": .3, \"topP\": 0.1}\n",
    "    \n",
    "    # Initialize the Bedrock client\n",
    "    client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=model_id, \n",
    "        messages=messages,\n",
    "        system=system, \n",
    "        inferenceConfig=inf_params\n",
    "    )\n",
    "    \n",
    "    # Print Response\n",
    "    output_message = response[\"output\"][\"message\"][\"content\"][0][\"text\"]\n",
    "\n",
    "    return output_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DSPy programming model proposes a systematic approach to designing AI pipelines by translating hand-based prompting techniques into declarative modules that carry natural-language typed signatures. These modules are task-adaptive components that can learn any particular text transformation, like answering a question or summarizing a paper. The model then parameterizes each module so that it can learn its desired behavior by iteratively bootstrapping useful demonstrations within the pipeline.\n"
     ]
    }
   ],
   "source": [
    "PRO_MODEL_ID = \"amazon.nova-pro-v1:0\"\n",
    "LITE_MODEL_ID = \"amazon.nova-lite-v1:0\"\n",
    "MICRO_MODEL_ID = \"amazon.nova-micro-v1:0\"\n",
    "\n",
    "response = send_images_to_model_using_converse(matched_items=matched_image_paths, query=text_query, model_id=PRO_MODEL_ID)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Building an Agentic RAG System with CrewAI\n",
    "\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, LLM\n",
    "from crewai_tools import tool\n",
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "PRO_MODEL_ID = \"us.amazon.nova-pro-v1:0\"\n",
    "LITE_MODEL_ID = \"us.amazon.nova-lite-v1:0\"\n",
    "MICRO_MODEL_ID = \"us.amazon.nova-micro-v1:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a web search tool  \n",
    "@tool('DuckDuckGoSearch')\n",
    "def search(search_query: str):\n",
    "    \"\"\"Search the web for information on a given topic\"\"\"\n",
    "    return DuckDuckGoSearchRun().run(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the LLM\n",
    "llm = LLM(model=PRO_MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an agent to search the web who is an expert in research papers and applied machine learning\n",
    "web_search_agent = Agent(\n",
    "    role=\"Web Search Agent\",\n",
    "    goal=\"An expert in research papers and applied machine learning\",\n",
    "    backstory=\"You are an expert in research papers and applied machine learning\",\n",
    "    verbose=True,\n",
    "    allow_delegation=False,\n",
    "    llm=llm,\n",
    "    tools=[search]\n",
    ")\n",
    "# Define a task to search the web\n",
    "web_search_task = Task(\n",
    "    description=\"Search {query} on the web for information on the given topic\",\n",
    "    expected_output=\"A list of web search results\",     \n",
    "    agent=web_search_agent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-11 17:44:21,848 - 8232306752 - __init__.py-__init__:537 - WARNING: Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mWeb Search Agent\u001b[00m\n",
      "\u001b[95m## Task:\u001b[00m \u001b[92mSearch What is the latest research in applied machine learning? on the web for information on the given topic\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mWeb Search Agent\u001b[00m\n",
      "\u001b[95m## Thought:\u001b[00m \u001b[92mThought: I need to search the web for the latest research in applied machine learning.\u001b[00m\n",
      "\u001b[95m## Using tool:\u001b[00m \u001b[92mDuckDuckGoSearch\u001b[00m\n",
      "\u001b[95m## Tool Input:\u001b[00m \u001b[92m\n",
      "\"{\\\"search_query\\\": \\\"latest research in applied machine learning\\\"}\"\u001b[00m\n",
      "\u001b[95m## Tool Output:\u001b[00m \u001b[92m\n",
      "ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills. lecar-lab/asap • • 3 Feb 2025. In the second stage, we deploy the policies in the real world and collect real-world data to train a delta (residual) action model that compensates for the dynamics mismatch. Lev Craig covers AI and machine learning as site editor for TechTarget's Enterprise AI site. Craig graduated from Harvard University with a bachelor's degree in English and has previously written about enterprise IT, software development and cybersecurity. Next Steps. The year in AI: Catch up on the top AI news of 2024 But we will incorporate the latest research whenever possible. Randy has just completed his annual survey of data, analytics, and AI executives, the 2025 AI & Data Leadership Executive Benchmark Survey , conducted by his educational firm, Data & AI Leadership Exchange; and Tom has worked on several surveys on generative AI and data, technology ... Some research on model reuse has been carried out in transfer learning 59, personalization 60, efficient searching 61 and multi-task learning 62. The key goals of model reuse methods include: The ... Read the latest Research articles in Machine learning from Scientific Reports ... Research on noise-induced hearing loss based on functional and structural MRI using machine learning methods ...\u001b[00m\n",
      "\n",
      "\n",
      "\u001b[1m\u001b[95m# Agent:\u001b[00m \u001b[1m\u001b[92mWeb Search Agent\u001b[00m\n",
      "\u001b[95m## Final Answer:\u001b[00m \u001b[92m\n",
      "1. **ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills**\n",
      "   - Source: lecar-lab/asap\n",
      "   - Date: 3 Feb 2025\n",
      "   - Description: This research focuses on aligning simulation and real-world physics to enable agile humanoid whole-body skills. It involves a two-stage process where policies are first trained in simulation and then deployed in the real world. Real-world data is collected to train a delta (residual) action model that compensates for the dynamics mismatch between simulation and reality.\n",
      "\n",
      "2. **Lev Craig's Coverage on AI and Machine Learning**\n",
      "   - Lev Craig is the site editor for TechTarget's Enterprise AI site, covering AI and machine learning. He graduated from Harvard University with a bachelor's degree in English and has written about enterprise IT, software development, and cybersecurity.\n",
      "\n",
      "3. **2025 AI & Data Leadership Executive Benchmark Survey**\n",
      "   - Conducted by Data & AI Leadership Exchange, this survey provides insights from data, analytics, and AI executives. It incorporates the latest research in the field.\n",
      "\n",
      "4. **Research on Model Reuse in Machine Learning**\n",
      "   - Various research efforts have been conducted in areas such as transfer learning, personalization, efficient searching, and multi-task learning. The key goals of model reuse methods include improving efficiency and performance in machine learning applications.\n",
      "\n",
      "5. **Latest Research Articles in Machine Learning from Scientific Reports**\n",
      "   - This includes research on noise-induced hearing loss based on functional and structural MRI using machine learning methods.\u001b[00m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a crew with the web search agent and task\n",
    "crew = Crew(\n",
    "    agents=[web_search_agent],\n",
    "    tasks=[web_search_task],\n",
    "    verbose=True\n",
    ")\n",
    "# Run the crew\n",
    "query = \"What is the latest research in applied machine learning?\"\n",
    "result = crew.kickoff(inputs={\"query\": query}) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adv-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
