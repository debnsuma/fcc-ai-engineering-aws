{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Integrating Visual Document Intelligence with Voice Response with ColPali, Bedrock and Strands Agents\n",
    "</h1>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"imgs/multimodal.png\" alt=\"Multimodal Diagram\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import qdrant_client\n",
    "import time\n",
    "import shutil\n",
    "import base64\n",
    "import ollama\n",
    "import boto3\n",
    "\n",
    "from io import BytesIO\n",
    "from huggingface_hub import login\n",
    "from colpali_engine.models import ColPali, ColPaliProcessor\n",
    "from qdrant_client.http import models\n",
    "from tqdm import tqdm \n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from IPython.display import Markdown\n",
    "from dotenv import load_dotenv\n",
    "from pdf2image import convert_from_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    Loading the PDF files (Dataset)\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to save the PDF files \n",
    "pdf_dir = \"pdf_data\"\n",
    "os.makedirs(pdf_dir, exist_ok=True)\n",
    "\n",
    "# Go to https://ncert.nic.in/textbook.php?jesc1=13-13 and download the PDF files and save them in the pdf_data directory "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Load the ColPali Multimodal Document Retrieval Model\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()  \n",
    "\n",
    "# Login using token from environment variable\n",
    "login(token=os.getenv('HUGGING_FACE_TOKEN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA/MPS/CPU is available\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"{device = }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code block does the following:\n",
    "\n",
    "1. First, it defines the model name [`vidore/colpali-v1.3`](https://huggingface.co/vidore/colpali-v1.3) which is a pre-trained model hosted on the Hugging Face Hub.\n",
    "\n",
    "2. Then it initializes two main components:\n",
    "\n",
    "   a. The `ColPaliProcessor`: This is responsible for:\n",
    "   - Processing and preparing images for the model\n",
    "   - Tokenizing text inputs\n",
    "   - Converting inputs into the format the model expects\n",
    "   \n",
    "   b. The `ColPali` model: This is the main multimodal model that:\n",
    "   - Can handle both image and text inputs\n",
    "   - Uses bfloat16 precision for memory efficiency\n",
    "   - Automatically maps to the best available device (GPU/CPU/MPS)\n",
    "   - Caches downloaded files locally to avoid re-downloading\n",
    "\n",
    "Both components use the same model name and cache directory to ensure consistency and efficiency in loading the pre-trained weights and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"vidore/colpali-v1.3\"\n",
    "\n",
    "colpali_model = ColPali.from_pretrained(\n",
    "                pretrained_model_name_or_path=model_name,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=device, \n",
    "                cache_dir=\"./model_cache\"\n",
    "            )\n",
    "\n",
    "colpali_processor = ColPaliProcessor.from_pretrained(\n",
    "                pretrained_model_name_or_path=model_name,\n",
    "                cache_dir=\"./model_cache\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "    Setup vector database\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, open a terminal and run the following to setup the vector database\n",
    "\n",
    "```\n",
    "docker run -p 6333:6333 -p 6334:6334 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage:z \\\n",
    "    qdrant/qdrant\n",
    "```\n",
    "\n",
    "Once that is done, you can check the [Qdrant Dashboard](http://localhost:6333/dashboard#/welcome) locally\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Qdrant client\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")\n",
    "\n",
    "# Get the collection info\n",
    "client.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collection name\n",
    "COLLECTION_NAME = \"class_X_science\"\n",
    "VECTOR_SIZE = 128\n",
    "\n",
    "# Check if collection exists\n",
    "collections = client.get_collections().collections\n",
    "collection_names = [collection.name for collection in collections]\n",
    "\n",
    "if COLLECTION_NAME not in collection_names:\n",
    "    # Create a collection only if it doesn't exist\n",
    "    client.create_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        on_disk_payload=True,\n",
    "        vectors_config=models.VectorParams(\n",
    "            size=VECTOR_SIZE,\n",
    "            distance=models.Distance.COSINE,\n",
    "            on_disk=True,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    print(f\"Collection '{COLLECTION_NAME}' created successfully.\")\n",
    "else:\n",
    "    print(f\"Collection '{COLLECTION_NAME}' already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the collection [here](http://localhost:6333/dashboard#/collections)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Store embeddings in vector database\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable parallelism in tokenizers to prevent potential issues and improve stability, especially in environments with limited resources.\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" \n",
    "\n",
    "# Wrapper function to convert PDFs into a dictionary of PIL images which will be used to create embeddings\n",
    "def convert_pdfs_to_images(pdf_folder, poppler_path=\"/opt/homebrew/bin\"):\n",
    "    \"\"\"Convert PDFs into a dictionary of PIL images.\"\"\"\n",
    "    pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "    all_images = []\n",
    "\n",
    "    for doc_id, pdf_file in enumerate(pdf_files):\n",
    "        pdf_path = os.path.join(pdf_folder, pdf_file)\n",
    "        images = convert_from_path(pdf_path, poppler_path=poppler_path)\n",
    "        \n",
    "        for page_num, image in enumerate(images):\n",
    "            all_images.append({\"doc_id\": doc_id, \"page_num\": page_num, \"image\": image.convert(\"RGB\")})\n",
    "\n",
    "    return all_images\n",
    "\n",
    "PDF_DIR = \"./pdf_data\"   # Change this to your actual folder path\n",
    "dataset = convert_pdfs_to_images(PDF_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "print(\"Generating embeddings and storing in Qdrant...\")\n",
    "\n",
    "with tqdm(total=len(dataset), desc=\"Indexing Progress\") as pbar:\n",
    "    for i in range(0, len(dataset), BATCH_SIZE):\n",
    "        batch = dataset[i : i + BATCH_SIZE]\n",
    "\n",
    "        # Extract images\n",
    "        images = [item[\"image\"] for item in batch]\n",
    "\n",
    "        # Process and encode images\n",
    "        with torch.no_grad():\n",
    "            batch_images = colpali_processor.process_images(images).to(colpali_model.device)\n",
    "            image_embeddings = colpali_model(**batch_images)\n",
    "\n",
    "        # Prepare points for Qdrant\n",
    "        points = []\n",
    "        for j, embedding in enumerate(image_embeddings):\n",
    "            points.append(\n",
    "                models.PointStruct(\n",
    "                    id=i + j,  # Use the batch index as the ID\n",
    "                    vector=embedding.tolist(),  # Convert to list\n",
    "                    payload={\n",
    "                        \"doc_id\": batch[j][\"doc_id\"],\n",
    "                        \"page_num\": batch[j][\"page_num\"],\n",
    "                        \"source\": \"pdf archive\",\n",
    "                    },  \n",
    "                )\n",
    "            )\n",
    "\n",
    "        # Upload points to Qdrant\n",
    "        try:\n",
    "            client.upsert(collection_name=COLLECTION_NAME, points=points)\n",
    "        except Exception as e:\n",
    "            print(f\"Error during upsert: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Update the progress bar\n",
    "        pbar.update(BATCH_SIZE)\n",
    "\n",
    "print(\"Indexing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Retrieval (Query Time)\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Our query\n",
    "query_text = \"What are the different Trophic levels\"\n",
    "\n",
    "# Step 2: Generate embeddings for the query\n",
    "with torch.no_grad():\n",
    "    text_embedding = colpali_processor.process_queries([query_text]).to(colpali_model.device)  \n",
    "    text_embedding = colpali_model(**text_embedding)\n",
    "\n",
    "token_query = text_embedding[0].cpu().float().numpy().tolist()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Step 3: Query the vector database\n",
    "query_result = client.query_points(collection_name=COLLECTION_NAME,\n",
    "                                   query=token_query,\n",
    "                                   limit=5,\n",
    "                                   search_params=models.SearchParams(\n",
    "                                   quantization=models.QuantizationSearchParams(\n",
    "                                   ignore=True,\n",
    "                                   rescore=True,\n",
    "                                   oversampling=2.0\n",
    "                                   )\n",
    "                               )\n",
    "                           )\n",
    "\n",
    "# Time taken to retrieve the results\n",
    "print(f\"Time taken = {(time.time()-start_time):.3f} s\")\n",
    "\n",
    "# Print the results\n",
    "query_result.points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a folder to save matched images\n",
    "MATCHED_IMAGES_DIR = \"matched_images\"\n",
    "\n",
    "# Delete all files and the directory itself if it exists\n",
    "if os.path.exists(MATCHED_IMAGES_DIR):\n",
    "    shutil.rmtree(MATCHED_IMAGES_DIR)\n",
    "\n",
    "os.makedirs(MATCHED_IMAGES_DIR)\n",
    "\n",
    "# Extract matched images from dataset based on query_result\n",
    "matched_images = []\n",
    "matched_images_path = []\n",
    "\n",
    "for result in query_result.points:\n",
    "    doc_id = result.payload[\"doc_id\"]\n",
    "    page_num = result.payload[\"page_num\"]\n",
    "\n",
    "    # Find the matching image in dataset\n",
    "    for item in dataset:\n",
    "        if item[\"doc_id\"] == doc_id and item[\"page_num\"] == page_num:\n",
    "            matched_images.append(item[\"image\"])\n",
    "\n",
    "            # Save the matched image\n",
    "            image_filename = os.path.join(MATCHED_IMAGES_DIR, f\"match_doc_{doc_id}_page_{page_num}.png\")\n",
    "            item[\"image\"].save(image_filename, \"PNG\")\n",
    "            matched_images_path.append(image_filename)\n",
    "            print(f\"Saved: {image_filename}\")\n",
    "\n",
    "            break\n",
    "\n",
    "print(\"\\n All matched images are saved in the 'matched_images' folder.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Visualizing the matched images\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def display_image_grid(images, num_cols=5, figsize=(15, 10)):\n",
    "    \"\"\"\n",
    "    Display a grid of images using matplotlib.\n",
    "    \n",
    "    Args:\n",
    "        images: List of images to display\n",
    "        num_cols: Number of columns in the grid (default: 8)\n",
    "        figsize: Figure size as tuple (width, height) (default: (15, 10))\n",
    "    \"\"\"\n",
    "    num_images = len(images)\n",
    "    num_rows = (num_images + num_cols - 1) // num_cols  # Calculate needed rows\n",
    "    \n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=figsize)\n",
    "    axes = axes.flat  # Flatten axes array for easier iteration\n",
    "    \n",
    "    # Display images\n",
    "    for i in range(num_cols * num_rows):\n",
    "        ax = axes[i]\n",
    "        if i < num_images:\n",
    "            ax.imshow(images[i])\n",
    "        ax.axis(\"off\")\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image_grid(matched_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference (via `Ollama`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pil_to_base64(pil_image):\n",
    "    buffered = BytesIO()\n",
    "    pil_image.save(buffered, format=\"PNG\")\n",
    "    img_str = base64.b64encode(buffered.getvalue()).decode()\n",
    "    return img_str\n",
    "\n",
    "image_list = [convert_pil_to_base64(item) for item in matched_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use ollama to generate the response from the matched images and the query\n",
    "model = ollama.generate(model=\"llava\", prompt=query_text, images=image_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(model.response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference (via `Bedrock`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_encode_image(image_path: str):\n",
    "\n",
    "    with open(image_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        \n",
    "    image_format = Image.open(image_path).format.lower()\n",
    "\n",
    "    message_content = {\n",
    "                    \"image\": {\n",
    "                        \"format\": image_format,\n",
    "                        \"source\": {\"bytes\": image_bytes},\n",
    "                    }\n",
    "                }\n",
    "    \n",
    "    return message_content\n",
    "\n",
    "\n",
    "def send_images_to_model_using_converse(matched_items: list, query: str, model_id: str):\n",
    "\n",
    "    system_prompt = 'You are a helpful assistant for question answering. Given the context, answer the question. Do not include any other text than the answer.'\n",
    "\n",
    "    image_list = []\n",
    "    for image_path in matched_items:\n",
    "        image_list.append({\n",
    "            \"image_path\": image_path, \n",
    "        })\n",
    "\n",
    "    content_list = []\n",
    "    for img in image_list:\n",
    "        message_content = read_and_encode_image(img['image_path'])\n",
    "        content_list.append(message_content)\n",
    "    \n",
    "    content_list.append({\"text\": query})\n",
    "    system = [ { \"text\": system_prompt } ]\n",
    "    \n",
    "    # Define a \"user\" message including both the image and a text prompt.\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": content_list,\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Configure the inference parameters.\n",
    "    inf_params = {\"temperature\": .3, \"maxTokens\": 5000}\n",
    "    \n",
    "    # Initialize the Bedrock client\n",
    "    client = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "\n",
    "    response = client.converse(\n",
    "        modelId=model_id, \n",
    "        messages=messages,\n",
    "        system=system, \n",
    "        inferenceConfig=inf_params\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "response = send_images_to_model_using_converse(matched_items=matched_images_path, query=query_text, model_id=model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response['output']['message']['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Building an Multimodal Agentic RAG System\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `Data Ingestion`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"imgs/1.png\" alt=\"Multimodal Diagram\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# `RAG Pipeline`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"imgs/2.png\" alt=\"Multimodal Diagram\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Building an Multimodal Agentic RAG using phoenix Agents\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"imgs/3.png\" alt=\"Multimodal Diagram\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating `Custom Tools` for phoenix Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent, tool\n",
    "\n",
    "# Create a Retrieval Custom Tool \n",
    "@tool\n",
    "def retrieve_from_qdrant(query: str):\n",
    "    \"\"\"\n",
    "    Retrieve the most relevant documents from Qdrant vector database\n",
    "    based on the given text query.\n",
    "    \n",
    "    Args:\n",
    "        query (str): The user query to search in the knowledge base.\n",
    "    \n",
    "    Returns:\n",
    "        list: List of paths to the matched images.\n",
    "    \"\"\"\n",
    "    global client, COLLECTION_NAME, colpali_processor, colpali_model\n",
    "\n",
    "    print(f\"ðŸ” Retrieving documents for query: {query}\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        text_embedding = colpali_processor.process_queries([query]).to(colpali_model.device)  \n",
    "        text_embedding = colpali_model(**text_embedding)\n",
    "\n",
    "    token_query = text_embedding[0].cpu().float().numpy().tolist()\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Perform search in Qdrant\n",
    "    query_result = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=token_query,\n",
    "        limit=5,\n",
    "        search_params=models.SearchParams(\n",
    "            quantization=models.QuantizationSearchParams(\n",
    "                ignore=True,\n",
    "                rescore=True,\n",
    "                oversampling=2.0\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    print(f\"â³ Query Time: {(time.time()-start_time):.3f} s\")\n",
    "\n",
    "    matched_images_path = []\n",
    "\n",
    "    # Define a folder to save matched images\n",
    "    MATCHED_IMAGES_DIR = \"matched_images\"\n",
    "    \n",
    "    # Delete all files and the directory itself if it exists\n",
    "    if os.path.exists(MATCHED_IMAGES_DIR):\n",
    "        shutil.rmtree(MATCHED_IMAGES_DIR)\n",
    "\n",
    "    os.makedirs(MATCHED_IMAGES_DIR)\n",
    "    \n",
    "    for result in query_result.points:\n",
    "        doc_id = result.payload[\"doc_id\"]\n",
    "        page_num = result.payload[\"page_num\"]\n",
    "\n",
    "        for item in dataset:\n",
    "            if item[\"doc_id\"] == doc_id and item[\"page_num\"] == page_num:\n",
    "                image_filename = os.path.join(\"matched_images\", f\"match_doc_{doc_id}_page_{page_num}.png\")\n",
    "                item[\"image\"].save(image_filename, \"PNG\")\n",
    "                matched_images_path.append(image_filename)\n",
    "\n",
    "                print(f\"Saved: {image_filename}\")\n",
    "                break  \n",
    "\n",
    "    print(\"\\n All matched images are saved in the 'matched_images' folder.\")\n",
    "    \n",
    "    return matched_images_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Defining the `Model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands import Agent\n",
    "from strands.models import BedrockModel\n",
    "from strands.models.ollama import OllamaModel\n",
    "\n",
    "from strands_tools import image_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Define the Ollama Model\n",
    "model_ollama = OllamaModel(\n",
    "    host=\"http://localhost:11434\",\n",
    "    model_id=\"llama3.2:latest\",\n",
    ")\n",
    "\n",
    "# Define the Bedrock Model\n",
    "model_bedrock = BedrockModel(\n",
    "    model_id=\"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n",
    "    max_tokens=64000\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an intelligent academic assistant named *Retrieval Agent*, designed to help students by retrieving the most relevant educational content from a structured knowledge base.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "- To interpret the student's natural language query and understand the intent.\n",
    "- To retrieve the most relevant **textbook pages, images, or diagrams** from the NCERT Class X Science knowledge base.\n",
    "- Use your tools effectively: \n",
    "  - `retrieve_from_qdrant` for semantic search across documents and image metadata.\n",
    "  - `image_reader` to extract text or captions from images, if needed for query interpretation.\n",
    "\n",
    "Guidelines:\n",
    "- You MUST return only the most relevant documents, images, or pages â€” prioritize **clarity, precision, and relevance**.\n",
    "- If the query is ambiguous, use your understanding to infer the most probable topic (e.g., \"How do lenses form images?\" should focus on *light chapter diagrams*).\n",
    "- Do NOT fabricate answers or try to explain concepts â€” your job is strictly to **retrieve**, not interpret or reason.\n",
    "- Return the content as a list of file paths or image encodings that can be passed to a reasoning agent.\n",
    "\n",
    "Your tone should remain academic and neutral. Ensure your output is concise, well-formatted, and structured to assist the next step in the pipeline (e.g., an answering or multimodal reasoning agent).\n",
    "\n",
    "Example Input:\n",
    "> \"Explain the difference between concave and convex lenses with diagrams.\"\n",
    "\n",
    "Example Output:\n",
    "> [\"images/fig2.png\", \"images/fig1.png\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Create the `Agent` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define the Agent \n",
    "retrieval_agent = Agent(model=model_bedrock, \n",
    "                        system_prompt=system_prompt, \n",
    "                        tools=[retrieve_from_qdrant, image_reader])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Send the `Query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Send the query to the Agent\n",
    "query_text = \"What are the different Trophic levels\"\n",
    "response = retrieval_agent(query_text)\n",
    "\n",
    "Markdown(response.message['content'][0]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Adding Voice Capability\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=\"imgs/4.png\" alt=\"Multimodal Diagram\">\n",
    "</p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- <p align=\"center\">\n",
    "  <img src=\"imgs/multimodal.png\" alt=\"Multimodal Diagram\">\n",
    "</p>\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding `Voice` Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from strands_tools import speak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an intelligent academic assistant named *Retrieval Agent*, designed to help students by retrieving the most relevant educational content from a structured knowledge base.\n",
    "\n",
    "Your primary responsibilities are:\n",
    "- To interpret the student's natural language query and understand the intent.\n",
    "- To retrieve the most relevant **textbook pages, images, or diagrams** from the NCERT Class X Science knowledge base.\n",
    "- Generate the response in the form of a text and also speak the answer to the user.\n",
    "- Use your tools effectively: \n",
    "  - `retrieve_from_qdrant` for semantic search across documents and image metadata.\n",
    "  - `image_reader` to extract text or captions from images, if needed for query interpretation.\n",
    "  - `speak` to speak the answer to the user.   \n",
    "\n",
    "Guidelines:\n",
    "- You MUST return only the most relevant documents, images, or pages â€” prioritize **clarity, precision, and relevance**.\n",
    "- If the query is ambiguous, use your understanding to infer the most probable topic (e.g., \"How do lenses form images?\" should focus on *light chapter diagrams*).\n",
    "- Do NOT fabricate answers or try to explain concepts â€” your job is strictly to **retrieve**, not interpret or reason.\n",
    "- Return the content as a list of file paths or image encodings that can be passed to a reasoning agent.\n",
    "\n",
    "Your tone should remain academic and neutral. Ensure your output is concise, well-formatted, and structured to assist the next step in the pipeline (e.g., an answering or multimodal reasoning agent).\n",
    "\n",
    "Example Input:\n",
    "> \"Explain the difference between concave and convex lenses with diagrams.\"\n",
    "\n",
    "Example Output:\n",
    "> [\"images/fig1.png\", \"images/fig2.png\"]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the `Agent` and send the `Query`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Retrieval Agent \n",
    "retrieval_agent_with_voice = Agent(model=model_bedrock, \n",
    "                                   system_prompt=system_prompt, \n",
    "                                   tools=[retrieve_from_qdrant, image_reader, speak])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_agent_with_voice(query_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"background: linear-gradient(to right, #ff6b6b, #4ecdc4); \n",
    "           color: white; \n",
    "           padding: 20px; \n",
    "           border-radius: 10px; \n",
    "           text-align: center; \n",
    "           font-family: Arial, sans-serif; \n",
    "           text-shadow: 2px 2px 4px rgba(0,0,0,0.5);\">\n",
    "  Thank you.\n",
    "</h1>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
